{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e54977e-dcb3-407b-b90d-0f3d8f7c7e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.layers import Dense, Activation, Input\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.python.framework.ops import disable_eager_execution\n",
    "import numpy as np\n",
    "import random\n",
    "import seaborn as sns\n",
    "\n",
    "disable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46efc861-f88d-4f49-a5df-ccde5e71147d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(object):\n",
    "    def __init__(self, alpha, beta, gamma=0.99, n_actions=4,\n",
    "                 layer1_size=1024, layer2_size=512, input_dims=32, memory_size=1000):\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.input_dims = input_dims\n",
    "        self.fc1_dims = layer1_size\n",
    "        self.fc2_dims = layer2_size\n",
    "        self.n_actions = n_actions\n",
    "\n",
    "        self.actor, self.critic, self.policy = self.build_actor_critic_network()\n",
    "        self.action_space = [i for i in range(n_actions)]\n",
    "        \n",
    "        self.memory = []\n",
    "        self.memory_size = memory_size\n",
    "        self.fig_count = 0\n",
    "\n",
    "    def build_actor_critic_network(self):\n",
    "        inputs = Input(shape=(self.input_dims,))\n",
    "        delta = Input(shape=[1])\n",
    "        dense1 = Dense(self.fc1_dims, activation='relu')(inputs)\n",
    "        dense2 = Dense(self.fc2_dims, activation='relu')(dense1)\n",
    "        probs = Dense(self.n_actions, activation='softmax')(dense2)\n",
    "        \n",
    "        \n",
    "        inputs_critic = Input(shape=(self.input_dims,))\n",
    "        dense1_critic = Dense(self.fc1_dims, activation='relu')(inputs_critic)\n",
    "        dense2_critic = Dense(self.fc2_dims, activation='relu')(dense1_critic)\n",
    "        q_values = Dense(self.n_actions, activation='linear')(dense2_critic)\n",
    "\n",
    "        def custom_loss(y_true, y_pred):\n",
    "            out = K.clip(y_pred, 1e-1, 1-1e-1)\n",
    "            log_lik = y_true*K.log(out)\n",
    "            \n",
    "            return K.sum(-log_lik*delta)\n",
    "\n",
    "        actor = Model(inputs=[inputs, delta], outputs=[probs])\n",
    "\n",
    "        actor.compile(optimizer=Adam(learning_rate=self.alpha), loss=custom_loss)\n",
    "\n",
    "        critic = Model(inputs=[inputs_critic], outputs=[q_values])\n",
    "\n",
    "        critic.compile(optimizer=Adam(learning_rate=self.beta), loss='mean_squared_error')\n",
    "\n",
    "        policy = Model(inputs=[inputs], outputs=[probs])\n",
    "\n",
    "        return actor, critic, policy\n",
    "\n",
    "    def choose_action(self, observation, ep = 0):\n",
    "        # state = observation[np.newaxis, :]\n",
    "        state = np.array([observation])\n",
    "        probabilities = self.policy.predict(state)[0]\n",
    "        action = np.random.choice(self.action_space, p=probabilities)\n",
    "        if random.random() < ep:\n",
    "            action = np.random.choice(self.action_space)\n",
    "\n",
    "        return action\n",
    "    \n",
    "    def addStateTransition(self, stateTransition):\n",
    "        if(len(self.memory) >= self.memory_size):\n",
    "            del self.memory[random.randint(0, len(self.memory) - 1)] #remove one element\n",
    "        self.memory.append(stateTransition)\n",
    "    \n",
    "    def stateTransition(self, gs0, action, reward, gs1, terminal):\n",
    "        st = [gs0, action, reward, gs1, terminal]\n",
    "        self.addStateTransition(st)\n",
    "    \n",
    "    def learn(self):\n",
    "        state_transitions = self.memory\n",
    "        random.shuffle(state_transitions)\n",
    "        \n",
    "        # state_transitions are lists of 5 tuples: current_state, action, reward, next_state, done/game_over\n",
    "        current_states = np.array([state_transition[0] for state_transition in state_transitions])\n",
    "        actions = np.array([state_transition[1] for state_transition in state_transitions])\n",
    "        rewards = np.array([state_transition[2] for state_transition in state_transitions])\n",
    "        next_states = np.array([state_transition[3] for state_transition in state_transitions])\n",
    "        dones = np.array([0 if state_transition[4] else 1 for state_transition in state_transitions])\n",
    "        \n",
    "        critic_value_next = np.max(self.critic.predict(next_states), axis=1)\n",
    "        critic_q_values = self.critic.predict(current_states)\n",
    "        \n",
    "        target = rewards + self.gamma * critic_value_next * dones\n",
    "        delta = target - np.max(critic_q_values, axis = 1)\n",
    "        \n",
    "        # for i in range(critic_q_values.shape[0]):\n",
    "        #     critic_q_values[i][actions[i]] = target[i]\n",
    "        critic_q_values[np.arange(critic_q_values.shape[0]), actions] = target\n",
    "        \n",
    "        actions_one_hot = np.zeros([actions.shape[0], self.n_actions])\n",
    "        actions_one_hot[np.arange(actions.shape[0]), actions] = 1\n",
    "        \n",
    "        history_actor = self.actor.fit([current_states, delta], actions_one_hot, verbose=0, epochs=1)\n",
    "        history_actor = history_actor.history['loss']\n",
    "\n",
    "        history_critic = self.critic.fit(current_states, critic_q_values, verbose=0, epochs=1)\n",
    "        history_critic = history_critic.history['loss']\n",
    "        \n",
    "        sns.lineplot(x = range(len(history_actor)), y = history_actor)\n",
    "        plt.savefig(f\"figures/fig_actor_{self.fig_count}.png\")\n",
    "        plt.clf()\n",
    "        \n",
    "        sns.lineplot(x = range(len(history_critic)), y = history_critic)\n",
    "        plt.savefig(f\"figures/fig_critic_{self.fig_count}.png\")\n",
    "        plt.clf()\n",
    "        self.fig_count += 1\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f622771-b93a-4f46-a77c-6cf8a1eb876f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
